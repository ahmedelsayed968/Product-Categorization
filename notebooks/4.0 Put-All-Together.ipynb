{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow. import Dataset\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "\n",
    "class ImageProcessor:\n",
    "    IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "    BATCH_SIZE = 32\n",
    "    CLASS_NAMES = None\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    @classmethod\n",
    "    def get_label(cls, file_path: pathlib.Path):\n",
    "        assert ImageProcessor.CLASS_NAMES is not None\n",
    "        parts = tf.strings.split(file_path, \"/\")\n",
    "        return tf.cast(parts[-2] == ImageProcessor.CLASS_NAMES, tf.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def decode_img(cls, img):\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return tf.image.resize(\n",
    "            img, [ImageProcessor.IMG_WIDTH, ImageProcessor.IMG_HEIGHT]\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def process_path(cls, file_path: pathlib.Path):\n",
    "        label = ImageProcessor.get_label(file_path)\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = ImageProcessor.decode_img(img)\n",
    "        return img, label\n",
    "\n",
    "    @classmethod\n",
    "    def prepare_for_training(cls, ds: tf.data.Dataset, shuffle_buffer_size=1000):\n",
    "\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "        ds = ds.batch(ImageProcessor.BATCH_SIZE)\n",
    "        ds = ds.prefetch(buffer_size=ImageProcessor.AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "\n",
    "class ImageDataSet:\n",
    "    def __init__(\n",
    "        self, path: str, train_size: float, test_size: float, val_size: float\n",
    "    ) -> None:\n",
    "        self.path = path\n",
    "        self.data_dir = pathlib.Path(path)\n",
    "        self.dataset_size = len(list(self.data_dir.glob(\"*/*.jpg\")))\n",
    "        self.list_ds = tf.data.Dataset.list_files(str(self.data_dir / \"*/*\"))\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        self.val = None\n",
    "        self.train_sz = int(train_size * self.dataset_size)\n",
    "        self.test_sz = int(test_size * self.dataset_size)\n",
    "        self.val_sz = int(val_size * self.dataset_size)\n",
    "        self.CLASS_NAMES = np.array([item.name for item in self.data_dir.glob(\"*\")])\n",
    "\n",
    "    def get_train_val_test(self, batch_size, width, height) -> Tuple[tf.data.Dataset]:\n",
    "        ImageProcessor.CLASS_NAMES = self.CLASS_NAMES\n",
    "        ImageProcessor.IMG_WIDTH, ImageProcessor.IMG_HEIGHT = width, height\n",
    "        ImageProcessor.BATCH_SIZE = batch_size\n",
    "        if not self.train:\n",
    "            self.train = self.list_ds.take(self.train_sz).map(\n",
    "                ImageProcessor.process_path\n",
    "            )\n",
    "            self.train = ImageProcessor.prepare_for_training(self.train)\n",
    "\n",
    "        remaining = self.list_ds.skip(self.train_sz)\n",
    "        if not self.val:\n",
    "            self.val = remaining.take(self.val_sz).map(ImageProcessor.process_path)\n",
    "            self.val = ImageProcessor.prepare_for_training(self.val)\n",
    "\n",
    "        if not self.test:\n",
    "            self.test = remaining.skip(self.val_sz).map(ImageProcessor.process_path)\n",
    "            self.test = ImageProcessor.prepare_for_training(self.test)\n",
    "        return self.train, self.val, self.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import callbacks\n",
    "import wandb\n",
    "import os\n",
    "import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def model_v1(num_classes: int, width: int, height: int, trainable: bool = False):\n",
    "    # Load Base Model\n",
    "    base_model = keras.applications.ResNet50V2(\n",
    "        include_top=False,  # Exclude ImageNet classifier at the top\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(width, height, 3),\n",
    "    )\n",
    "    # Freeze all parameters of the base model\n",
    "    base_model.trainable = trainable\n",
    "    inputs = keras.Input(shape=(width, height, 3))\n",
    "    # Apply specific pre-processing function for ResNet v2\n",
    "    x = keras.applications.resnet_v2.preprocess_input(inputs)\n",
    "    # Keep base model batch normalization layers in inference mode (instead of training mode)\n",
    "    x = base_model(x, training=False)\n",
    "    # Rebuild top layers\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # Average pooling operation\n",
    "    x = layers.BatchNormalization()(x)  # Introduce batch norm\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    # Flattening to final layer - Dense classifier with 37 units (multi-class classification)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def model_v2(num_classes: int, width: int, height: int, trainable: bool = False):\n",
    "    # Load Base Model\n",
    "    base_model = keras.applications.ResNet50V2(\n",
    "        include_top=False,  # Exclude ImageNet classifier at the top\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(width, height, 3),\n",
    "    )\n",
    "    # Freeze all parameters of the base model\n",
    "    base_model.trainable = trainable\n",
    "    inputs = keras.Input(shape=(width, height, 3))\n",
    "    # Apply specific pre-processing function for ResNet v2\n",
    "    x = keras.applications.resnet_v2.preprocess_input(inputs)\n",
    "    # Keep base model batch normalization layers in inference mode (instead of training mode)\n",
    "    x = base_model(x, training=False)\n",
    "    # Rebuild top layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    x = layers.Dense(units=64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    x = layers.Dense(units=32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def model_v3(num_classes: int, width: int, height: int, trainable: bool = False):\n",
    "    # Load Base Model\n",
    "    base_model = keras.applications.VGG19(\n",
    "        include_top=False,  # Exclude ImageNet classifier at the top\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(width, height, 3),\n",
    "    )\n",
    "    # Freeze all parameters of the base model\n",
    "    base_model.trainable = trainable\n",
    "    inputs = keras.Input(shape=(width, height, 3))\n",
    "    # Apply specific pre-processing function for ResNet v2\n",
    "    x = keras.applications.vgg19.preprocess_input(inputs)\n",
    "    # Keep base model batch normalization layers in inference mode (instead of training mode)\n",
    "    x = base_model(x, training=False)\n",
    "    # Rebuild top layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    x = layers.Dense(units=128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    x = layers.Dense(units=64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    x = layers.Dense(units=64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    x = layers.Dense(units=32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    x = layers.Dense(units=32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def model_V4(num_classes: int, width: int, height: int, trainable: bool = False):\n",
    "    # Load Base Model\n",
    "    base_model = keras.applications.VGG19(\n",
    "        include_top=False,  # Exclude ImageNet classifier at the top\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(width, height, 3),\n",
    "    )\n",
    "    # Freeze all parameters of the base model\n",
    "    base_model.trainable = trainable\n",
    "    inputs = keras.Input(shape=(width, height, 3))\n",
    "    # Apply specific pre-processing function for ResNet v2\n",
    "    x = keras.applications.vgg19.preprocess_input(inputs)\n",
    "    # Keep base model batch normalization layers in inference mode (instead of training mode)\n",
    "    x = base_model(x, training=False)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # Average pooling operation\n",
    "    x = layers.BatchNormalization()(x)  # Introduce batch norm\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "    x = layers.Dense(32)(x)\n",
    "    x = layers.BatchNormalization()(x)  # Introduce batch norm\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def run(\n",
    "    model: tf.keras.Model,\n",
    "    epochs: int,\n",
    "    train_ds: tf.data.Dataset,\n",
    "    val_ds: tf.data.Dataset,\n",
    "    config_logs: Dict[str, Any],\n",
    "    checkpoint_filename: str,\n",
    "    save_dir: str,\n",
    "    compile_config: Dict[str, Any],\n",
    "    wandb_config: Dict,\n",
    "    hub_token: str = None,\n",
    "    hub_model_id: str = None,\n",
    "):\n",
    "\n",
    "    run = wandb.init(\n",
    "        sync_tensorboard=True, reinit=True, **config_logs, config=wandb_config\n",
    "    )\n",
    "    model.compile(**compile_config)\n",
    "    earlystopping = callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", mode=\"min\", patience=5, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Persist the model as checkpoint\n",
    "    checkpoint_dir = os.path.join(save_dir, checkpoint_filename)\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    #     checkpoint = callbacks.ModelCheckpoint(filepath=os.path.join(checkpoint_dir, checkpoint_filename,'model.keras'),\n",
    "    #                                            monitor=\"val_loss\",\n",
    "    #                                            verbose=1,\n",
    "    #                                            save_best_only=True,\n",
    "    #                                            save_weights_only=False)\n",
    "    #     #Push to Hub Callback\n",
    "    #     model_push_to_hub = PushToHubCallback(output_dir=checkpoint_dir,\n",
    "    #                                             save_strategy= \"epoch\",\n",
    "    #                                             hub_model_id = hub_model_id,\n",
    "    #                                             hub_token= hub_token,\n",
    "    #                                             checkpoint = True\n",
    "    #                                             )\n",
    "    # Tensorboard Tracking run Callback\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, histogram_freq=1\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        verbose=1,\n",
    "        callbacks=[earlystopping, tensorboard_callback],\n",
    "    )\n",
    "    run.finish()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "    plt.savefig(f\"history-{str(uuid.uuid1())}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args = {\n",
    "    \"path\": \"/kaggle/input/amazon-products-images-v2\",\n",
    "    \"train_size\": 0.8,\n",
    "    \"val_size\": 0.1,\n",
    "    \"test_size\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "dataset = ImageDataSet(**dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_args = {\"batch_size\": 16, \"width\": 224, \"height\": 224}\n",
    "train, val, test = dataset.get_train_val_test(**train_split_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "model = model_V4(\n",
    "    len(ImageProcessor.CLASS_NAMES),\n",
    "    train_split_args[\"width\"],\n",
    "    train_split_args[\"height\"],\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "metrics = [\n",
    "    tf.keras.metrics.CategoricalCrossentropy(name=\"categorical_crossentropy\"),\n",
    "    tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.Precision(name=\"precision1\", top_k=1),\n",
    "    tf.keras.metrics.Precision(name=\"precision3\", top_k=3),\n",
    "    tf.keras.metrics.Recall(name=\"recall1\", top_k=1),\n",
    "    tf.keras.metrics.Recall(name=\"recall3\", top_k=3),\n",
    "    tf.keras.metrics.F1Score(average=\"macro\", name=\"f1_score\"),\n",
    "]\n",
    "\n",
    "project = \"Slash\"\n",
    "id_ = f\"V2-{str(uuid.uuid4())}\"\n",
    "save_dir, checkpoint_filename = \"./model\", \"model-\" + str(datetime.datetime.now())\n",
    "config_logs = {\"project\": project, \"id\": id_}\n",
    "\n",
    "compile_config = {\"optimizer\": optimizer, \"loss\": loss, \"metrics\": metrics}\n",
    "wandb_config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": ImageProcessor.BATCH_SIZE,\n",
    "    \"architecture\": \"ModelV4\",\n",
    "    \"dataset\": \"V2\",\n",
    "}\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "hub_token = user_secrets.get_secret(\"HF\")\n",
    "# hub_model_id = f\"{config['architecture']}-{config['dataset']}-{config['epochs']}\"\n",
    "history1 = run(\n",
    "    model,\n",
    "    epochs,\n",
    "    train,\n",
    "    val,\n",
    "    config_logs,\n",
    "    checkpoint_filename,\n",
    "    save_dir,\n",
    "    compile_config,\n",
    "    wandb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"/kaggle/working/model/2024-03-15 09:43:28.386710/2024-03-15 09:43:28.386710model.keras\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path: str, model: tf.keras.models.Model) -> None:\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_json()\n",
    "    if not os.path.exists(os.path.join(path, \"model.json\")):\n",
    "        os.makedirs(path)\n",
    "    with open(os.path.join(path, \"model.json\"), \"w\") as json_file:\n",
    "        json_file.write(model_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(os.path.join(path, \"model.weights.h5\"))\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "save_model(\"/kaggle/working/saved/models/v1\", loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4600352,
     "sourceId": 7845849,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
